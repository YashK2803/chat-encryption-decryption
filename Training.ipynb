{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z58kd2Ui2qM0",
        "outputId": "12a58ba3-66ea-41bc-a2fe-7e1cfd5e6ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example synthetic word: ritelessness                                                    \n",
            "Epoch [500/20000], Loss: 0.3237, Accuracy: 90.48%\n",
            "Epoch [1000/20000], Loss: 0.1287, Accuracy: 96.34%\n",
            "Epoch [1500/20000], Loss: 0.1238, Accuracy: 97.07%\n",
            "Epoch [2000/20000], Loss: 0.0909, Accuracy: 97.90%\n",
            "Epoch [2500/20000], Loss: 0.0655, Accuracy: 98.78%\n",
            "Epoch [3000/20000], Loss: 0.0575, Accuracy: 98.63%\n",
            "Epoch [3500/20000], Loss: 0.0654, Accuracy: 98.54%\n",
            "Epoch [4000/20000], Loss: 0.0637, Accuracy: 98.34%\n",
            "Epoch [4500/20000], Loss: 0.0364, Accuracy: 99.17%\n",
            "Epoch [5000/20000], Loss: 0.0610, Accuracy: 98.78%\n",
            "Epoch [5500/20000], Loss: 0.0507, Accuracy: 98.93%\n",
            "Epoch [6000/20000], Loss: 0.0416, Accuracy: 99.02%\n",
            "Epoch [6500/20000], Loss: 0.0553, Accuracy: 98.73%\n",
            "Epoch [7000/20000], Loss: 0.0651, Accuracy: 98.24%\n",
            "Epoch [7500/20000], Loss: 0.0484, Accuracy: 99.07%\n",
            "Epoch [8000/20000], Loss: 0.0407, Accuracy: 99.22%\n",
            "Epoch [8500/20000], Loss: 0.0497, Accuracy: 98.73%\n",
            "Epoch [9000/20000], Loss: 0.0431, Accuracy: 98.97%\n",
            "Epoch [9500/20000], Loss: 0.0422, Accuracy: 99.02%\n",
            "Epoch [10000/20000], Loss: 0.0407, Accuracy: 99.12%\n",
            "Epoch [10500/20000], Loss: 0.0446, Accuracy: 98.97%\n",
            "Epoch [11000/20000], Loss: 0.0439, Accuracy: 98.83%\n",
            "Epoch [11500/20000], Loss: 0.0331, Accuracy: 99.22%\n",
            "Epoch [12000/20000], Loss: 0.0463, Accuracy: 99.07%\n",
            "Epoch [12500/20000], Loss: 0.0270, Accuracy: 99.32%\n",
            "Epoch [13000/20000], Loss: 0.0324, Accuracy: 99.12%\n",
            "Epoch [13500/20000], Loss: 0.0316, Accuracy: 99.32%\n",
            "Epoch [14000/20000], Loss: 0.0322, Accuracy: 98.97%\n",
            "Epoch [14500/20000], Loss: 0.0355, Accuracy: 99.17%\n",
            "Epoch [15000/20000], Loss: 0.0426, Accuracy: 98.88%\n",
            "Epoch [15500/20000], Loss: 0.0366, Accuracy: 98.97%\n",
            "Epoch [16000/20000], Loss: 0.0304, Accuracy: 99.17%\n",
            "Epoch [16500/20000], Loss: 0.0318, Accuracy: 99.17%\n",
            "Epoch [17000/20000], Loss: 0.0373, Accuracy: 99.27%\n",
            "Epoch [17500/20000], Loss: 0.0275, Accuracy: 99.22%\n",
            "Epoch [18000/20000], Loss: 0.0241, Accuracy: 99.22%\n",
            "Epoch [18500/20000], Loss: 0.0410, Accuracy: 98.97%\n",
            "Epoch [19000/20000], Loss: 0.0358, Accuracy: 99.12%\n",
            "Epoch [19500/20000], Loss: 0.0303, Accuracy: 99.32%\n",
            "Epoch [20000/20000], Loss: 0.0383, Accuracy: 99.02%\n",
            "\n",
            "Original Synthetic Word:\n",
            "examplelongword\n",
            "\n",
            "Decrypted Text from Clean Input:\n",
            "examplelontror                                                  \n",
            "\n",
            "Tampered Word:\n",
            "eCHmplelonXwo\fd\n",
            "\n",
            "Decrypted Text from Tampered Input:\n",
            "ecemplelontaoee                                                 \n",
            "\n",
            "Decryption Accuracy on Tampered Input: 90.62%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download('words', quiet=True)\n",
        "from nltk.corpus import words\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "MAX_LENGTH = 64\n",
        "\n",
        "# Defining vocabulary and mappings\n",
        "vocab = list(string.printable)\n",
        "vocab_size = len(vocab)\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
        "idx_to_char = {idx: ch for idx, ch in enumerate(vocab)}\n",
        "\n",
        "def text_to_tensor(text, max_length=MAX_LENGTH):\n",
        "    text = text[:max_length]\n",
        "    text = text.ljust(max_length)\n",
        "    indices = [char_to_idx.get(ch, 0) for ch in text]\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "def tensor_to_text(tensor):\n",
        "    indices = tensor.cpu().numpy().tolist()\n",
        "    return ''.join([idx_to_char.get(idx, '') for idx in indices])\n",
        "\n",
        "def tamper_text_message(original_text, tamper_strength=0.1):\n",
        "    text_length = len(original_text)\n",
        "    num_tampered_chars = int(text_length * tamper_strength)\n",
        "    tampered_indices = random.sample(range(text_length), num_tampered_chars)\n",
        "    tampered_text = list(original_text)\n",
        "    for idx in tampered_indices:\n",
        "        tampered_text[idx] = random.choice(string.printable)\n",
        "    return ''.join(tampered_text)\n",
        "\n",
        "# Model\n",
        "class TextAutoencoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, seq_length):\n",
        "        super(TextAutoencoder, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(seq_length * embed_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, hidden_dim),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, seq_length * embed_dim),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embedding(x)\n",
        "        encoded = self.encoder(embed)\n",
        "        decoded = self.decoder(encoded)\n",
        "        decoded = decoded.view(-1, self.seq_length, self.embed_dim)\n",
        "        logits = self.output_layer(decoded)\n",
        "        return logits\n",
        "\n",
        "# Training Dataset\n",
        "large_word_bank = [word for word in words.words() if word.isalpha() and len(word) <= 16]\n",
        "\n",
        "def generate_synthetic_word():\n",
        "    word = random.choice(large_word_bank)\n",
        "    return word.ljust(MAX_LENGTH)[:MAX_LENGTH]\n",
        "\n",
        "synthetic_dataset = [generate_synthetic_word() for _ in range(5000)]\n",
        "print(\"Example synthetic word:\", synthetic_dataset[0])\n",
        "\n",
        "# Training Parameters\n",
        "embed_dim = 32\n",
        "hidden_dim = 64\n",
        "seq_length = MAX_LENGTH\n",
        "learning_rate = 0.0005\n",
        "num_epochs = 20000\n",
        "batch_size = 32\n",
        "\n",
        "model = TextAutoencoder(vocab_size, embed_dim, hidden_dim, seq_length)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def get_batch(dataset, batch_size):\n",
        "    batch_texts = random.sample(dataset, batch_size)\n",
        "    inputs = [text_to_tensor(tamper_text_message(text, tamper_strength=0.1)) for text in batch_texts]\n",
        "    targets = [text_to_tensor(text) for text in batch_texts]\n",
        "    return torch.stack(inputs), torch.stack(targets)\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_batch, target_batch = get_batch(synthetic_dataset, batch_size)\n",
        "    train_batch = train_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(train_batch)\n",
        "    loss = criterion(logits.view(-1, vocab_size), target_batch.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        preds = torch.argmax(logits, dim=2)\n",
        "        accuracy = (preds == target_batch).float().mean().item()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_word = \"examplelongword\"\n",
        "    print(\"\\nOriginal Synthetic Word:\")\n",
        "    print(test_word)\n",
        "\n",
        "    clean_tensor = text_to_tensor(test_word).unsqueeze(0).to(device)\n",
        "    clean_logits = model(clean_tensor)\n",
        "    clean_pred = torch.argmax(clean_logits, dim=2).squeeze(0)\n",
        "    clean_decrypted = tensor_to_text(clean_pred)\n",
        "    print(\"\\nDecrypted Text from Clean Input:\")\n",
        "    print(clean_decrypted)\n",
        "\n",
        "    tampered_word = tamper_text_message(test_word, tamper_strength=0.3)\n",
        "    tampered_tensor = text_to_tensor(tampered_word).unsqueeze(0).to(device)\n",
        "    tampered_logits = model(tampered_tensor)\n",
        "    tampered_pred = torch.argmax(tampered_logits, dim=2).squeeze(0)\n",
        "    tampered_decrypted = tensor_to_text(tampered_pred)\n",
        "    tampered_acc = (tampered_pred == text_to_tensor(test_word).to(device)).float().mean().item()\n",
        "\n",
        "    print(\"\\nTampered Word:\")\n",
        "    print(tampered_word)\n",
        "    print(\"\\nDecrypted Text from Tampered Input:\")\n",
        "    print(tampered_decrypted)\n",
        "    print(f\"\\nDecryption Accuracy on Tampered Input: {tampered_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"text_autoencoder_final.pth\")"
      ],
      "metadata": {
        "id": "kCx2Jz956ES9"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}